Road map:
--> Random walk in latent space with GAN
    Style transfer
    Texture transfer
    ?

Current sprint:
--> Baseline GAN from tutorial (https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)
    Random walk in latent space
    Higher resolution images

Current task:
    Get training to run
    Track generator output on fixed latents
--> Use metrics/plotting from meta
    Save results
    Tests
    Refactoring

Notes for current sprint/task:
- Ideas for better training:
  - More data
  - Longer training time
  - Hyperparameter tuning
  - Newer GAN techniques

----------------------------------------------------------------------------------------

Future plans:

Refactoring:
- Make number of generator/discriminator layers options
- Initialization options
- Optimizer options

----------------------------------------------------------------------------------------

Reading:

- Read through GAN papers
  - Generative Adversarial Networks (Goodfellow)
  - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial
    Networks (Radford)
  - BigGAN?
  - BigBiGAN?
  - Look for more recent GAN stuff
  - Look into GAN literature about this point: Why, when training the generator, do we
    only compute the loss using fake images? Would it not be better to train the
    generator so that the discriminator classifies fake images as real as well as real
    images as fake?
- Read through VAE literature (especially the recent NVAE)
- Read distill.pub articles about image generation

----------------------------------------------------------------------------------------

Misc:

Flowers dataset came from: https://www.kaggle.com/alxmamaev/flowers-recognition
